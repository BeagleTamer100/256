{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeagleTamer100/256/blob/main/Clustering_Corona.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsforecast\n",
        "!pip install catboost\n",
        "!pip install mlforecast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRGjIuyAVhIt",
        "outputId": "f9707005-65c1-47d4-cb86-e3fb62410ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting statsforecast\n",
            "  Downloading statsforecast-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from statsforecast) (3.1.1)\n",
            "Collecting coreforecast>=0.0.12 (from statsforecast)\n",
            "  Downloading coreforecast-0.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: numba>=0.55.0 in /usr/local/lib/python3.11/dist-packages (from statsforecast) (0.61.0)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from statsforecast) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.11/dist-packages (from statsforecast) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from statsforecast) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from statsforecast) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from statsforecast) (4.67.1)\n",
            "Collecting fugue>=0.8.1 (from statsforecast)\n",
            "  Downloading fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting utilsforecast>=0.1.4 (from statsforecast)\n",
            "  Downloading utilsforecast-0.2.12-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3 in /usr/local/lib/python3.11/dist-packages (from statsforecast) (3.5.0)\n",
            "Collecting triad>=0.9.7 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.55.0->statsforecast) (0.44.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.5->statsforecast) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.5->statsforecast) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.5->statsforecast) (2025.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->statsforecast) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->statsforecast) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->statsforecast) (1.17.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (17.0.0)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (2024.10.0)\n",
            "Collecting fs (from triad>=0.9.7->fugue>=0.8.1->statsforecast)\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting appdirs~=1.4.3 (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast) (75.1.0)\n",
            "Downloading statsforecast-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.4/354.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coreforecast-0.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.8/275.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fugue-0.9.1-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utilsforecast-0.2.12-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading adagio-0.2.6-py3-none-any.whl (19 kB)\n",
            "Downloading triad-0.9.8-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: appdirs, fs, coreforecast, utilsforecast, triad, adagio, fugue, statsforecast\n",
            "Successfully installed adagio-0.2.6 appdirs-1.4.4 coreforecast-0.0.15 fs-2.4.16 fugue-0.9.1 statsforecast-2.0.1 triad-0.9.8 utilsforecast-0.2.12\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n",
            "Collecting mlforecast\n",
            "  Downloading mlforecast-1.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from mlforecast) (3.1.1)\n",
            "Requirement already satisfied: coreforecast>=0.0.15 in /usr/local/lib/python3.11/dist-packages (from mlforecast) (0.0.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from mlforecast) (2024.10.0)\n",
            "Collecting optuna (from mlforecast)\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from mlforecast) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from mlforecast) (1.6.1)\n",
            "Requirement already satisfied: utilsforecast>=0.2.9 in /usr/local/lib/python3.11/dist-packages (from mlforecast) (0.2.12)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from coreforecast>=0.0.15->mlforecast) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from utilsforecast>=0.2.9->mlforecast) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->mlforecast) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->mlforecast) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->mlforecast) (2025.1)\n",
            "Collecting alembic>=1.5.0 (from optuna->mlforecast)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna->mlforecast)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->mlforecast) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->mlforecast) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->mlforecast) (3.5.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->mlforecast)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->mlforecast) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->mlforecast) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->mlforecast) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna->mlforecast) (3.0.2)\n",
            "Downloading mlforecast-1.0.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna, mlforecast\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 mlforecast-1.0.2 optuna-4.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4aBv97o8eAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a228bdca-c33d-47fa-e224-f97e03f7db3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# General-purpose libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, mean_squared_error\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Forecasting libraries\n",
        "from statsforecast.models import (\n",
        "    SeasonalNaive, AutoTheta, CrostonClassic, CrostonOptimized,\n",
        "    ADIDA, CrostonSBA, IMAPA, TSB\n",
        ")\n",
        "from statsforecast.core import StatsForecast\n",
        "from mlforecast import MLForecast\n",
        "from mlforecast.lag_transforms import RollingMean\n",
        "\n",
        "# Machine learning regressors for forecasting\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set output directory\n",
        "output_dir = 'Corona'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "metadata": {
        "id": "ctUjn_DOjNVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('Customer Order Quantity_Dispatched Quantity_cleaned.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d.%m.%Y')"
      ],
      "metadata": {
        "id": "URqj82W787S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create complete date range for each product\n",
        "def create_complete_timeseries(df):\n",
        "    # Get min and max dates\n",
        "    min_date = df['Date'].min()\n",
        "    max_date = df['Date'].max()\n",
        "\n",
        "    # Create complete date range\n",
        "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "\n",
        "    # Get unique products\n",
        "    products = df['Product ID'].unique()\n",
        "\n",
        "    # Create empty list to store results\n",
        "    complete_data = []\n",
        "\n",
        "    # For each product, create complete time series\n",
        "    for product in products:\n",
        "        product_data = df[df['Product ID'] == product]\n",
        "\n",
        "        # Create template with all dates\n",
        "        template = pd.DataFrame(date_range, columns=['Date'])\n",
        "        template['Product ID'] = product\n",
        "        template['Product Name'] = product_data['Product Name'].iloc[0]\n",
        "\n",
        "        # Merge with actual data\n",
        "        merged = pd.merge(template, product_data[['Date', 'Customer Order Quantity']],\n",
        "                         on='Date', how='left')\n",
        "\n",
        "        # Fill NaN with 0\n",
        "        merged['Customer Order Quantity'] = merged['Customer Order Quantity'].fillna(0)\n",
        "\n",
        "        complete_data.append(merged)\n",
        "\n",
        "    # Combine all products\n",
        "    result = pd.concat(complete_data, ignore_index=True)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "nzsfs-MVjTCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate additional metrics for clustering\n",
        "def calculate_metrics(df):\n",
        "    metrics = []\n",
        "\n",
        "    for product in df['Product ID'].unique():\n",
        "        product_data = df[df['Product ID'] == product]\n",
        "        demand = product_data['Customer Order Quantity'].values\n",
        "\n",
        "        # Basic metrics\n",
        "        avg_demand = np.mean(demand) + np.finfo(float).eps\n",
        "        std_demand = np.std(demand)\n",
        "        cv = std_demand / avg_demand\n",
        "\n",
        "        # Calculate demand frequency\n",
        "        non_zero_demands = demand[demand > 0]\n",
        "        demand_frequency = len(non_zero_demands) / len(demand)\n",
        "\n",
        "        # Calculate average order size when orders occur\n",
        "        avg_order_size = np.mean(non_zero_demands) if len(non_zero_demands) > 0 else 0\n",
        "\n",
        "        # Calculate peak-to-average ratio\n",
        "        peak_demand = np.max(demand)\n",
        "        peak_to_avg = peak_demand / avg_demand if avg_demand > 0 else 0\n",
        "\n",
        "        # Calculate demand variability\n",
        "        rolling_mean = pd.Series(demand).rolling(window=7).mean()\n",
        "        demand_variability = np.std(rolling_mean) / np.mean(rolling_mean) if np.mean(rolling_mean) > 0 else 0\n",
        "\n",
        "        # Calculate intervals between orders\n",
        "        order_dates = np.where(demand > 0)[0]\n",
        "        if len(order_dates) > 1:\n",
        "            intervals = np.diff(order_dates)\n",
        "            avg_interval = np.mean(intervals)\n",
        "            interval_cv = np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else 0\n",
        "        else:\n",
        "            avg_interval = len(demand)\n",
        "            interval_cv = 0\n",
        "\n",
        "        zero_percentage = (demand == 0).mean() * 100\n",
        "\n",
        "        metrics.append({\n",
        "            'Product ID': product,\n",
        "            'Product Name': product_data['Product Name'].iloc[0],\n",
        "            'Average Demand': avg_demand,\n",
        "            'CV': cv,\n",
        "            'Average Interval': avg_interval,\n",
        "            'Zero Percentage': zero_percentage,\n",
        "            'Demand Frequency': demand_frequency,\n",
        "            'Average Order Size': avg_order_size,\n",
        "            'Peak to Average': peak_to_avg,\n",
        "            'Demand Variability': demand_variability,\n",
        "            'Interval CV': interval_cv\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(metrics)\n"
      ],
      "metadata": {
        "id": "hVNCaRRtjWYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create complete timeseries and calculate metrics\n",
        "complete_df = create_complete_timeseries(df)\n",
        "metrics_df = calculate_metrics(complete_df)\n",
        "\n",
        "\n",
        "# Modified try_clustering_methods function to provide more detailed results\n",
        "def try_clustering_methods(X_scaled, metrics_df):\n",
        "    results = {}\n",
        "\n",
        "    # 1. K-means with different k\n",
        "    print(\"\\nTesting K-means clustering:\")\n",
        "    for k in range(3, 7):\n",
        "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "        labels = kmeans.fit_predict(X_scaled)\n",
        "        score = silhouette_score(X_scaled, labels)\n",
        "        results[f'KMeans-{k}'] = (labels, score)\n",
        "        print(f\"K={k}: Silhouette Score = {score:.3f}, Cluster sizes = {np.bincount(labels)}\")\n",
        "\n",
        "    # 2. DBSCAN with different parameters\n",
        "    print(\"\\nTesting DBSCAN clustering:\")\n",
        "    for eps in [0.5, 0.7, 1.0]:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=3)\n",
        "        labels = dbscan.fit_predict(X_scaled)\n",
        "        # Handle noise points (-1 labels) for visualization\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        if n_clusters > 1:  # Only calculate score if more than one cluster\n",
        "            # Filter out noise points for silhouette score\n",
        "            non_noise_mask = labels != -1\n",
        "            score = silhouette_score(X_scaled[non_noise_mask], labels[non_noise_mask])\n",
        "            results[f'DBSCAN-{eps}'] = (labels, score)\n",
        "            # Count occurrences including noise points\n",
        "            unique, counts = np.unique(labels, return_counts=True)\n",
        "            cluster_sizes = dict(zip(unique, counts))\n",
        "            print(f\"eps={eps}: Silhouette Score = {score:.3f}\")\n",
        "            print(f\"Cluster sizes (including noise points): {cluster_sizes}\")\n",
        "\n",
        "    # 3. Hierarchical clustering\n",
        "    print(\"\\nTesting Hierarchical clustering:\")\n",
        "    for k in range(3, 7):\n",
        "        hc = AgglomerativeClustering(n_clusters=k)\n",
        "        labels = hc.fit_predict(X_scaled)\n",
        "        score = silhouette_score(X_scaled, labels)\n",
        "        results[f'Hierarchical-{k}'] = (labels, score)\n",
        "        print(f\"K={k}: Silhouette Score = {score:.3f}, Cluster sizes = {np.bincount(labels)}\")\n",
        "\n",
        "    # Find best method\n",
        "    best_method = max(results.items(), key=lambda x: x[1][1])\n",
        "    print(f\"\\nBest clustering method: {best_method[0]}\")\n",
        "    print(f\"Best silhouette score: {best_method[1][1]:.3f}\")\n",
        "\n",
        "    return best_method[1][0], results\n",
        "\n",
        "\n",
        "# Prepare data for clustering using selected features\n",
        "clustering_features = [\n",
        "    'CV',\n",
        "    'Average Interval',\n",
        "    'Average Demand',\n",
        "    'Average Order Size'\n",
        "]\n",
        "\n",
        "\n",
        "print(\"\\nUsing clustering features:\", clustering_features)\n",
        "\n",
        "\n",
        "# Prepare the feature matrix\n",
        "X = metrics_df[clustering_features].values\n",
        "\n",
        "\n",
        "# Use RobustScaler for better handling of outliers\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "# Print feature ranges after scaling\n",
        "print(\"\\nFeature ranges after scaling:\")\n",
        "for i, feature in enumerate(clustering_features):\n",
        "    print(f\"{feature}: {X_scaled[:, i].min():.2f} to {X_scaled[:, i].max():.2f}\")\n",
        "\n",
        "\n",
        "# Try different clustering methods and get best results\n",
        "best_labels, all_results = try_clustering_methods(X_scaled, metrics_df)\n",
        "metrics_df['Cluster'] = best_labels\n",
        "\n",
        "\n",
        "# Print cluster characteristics\n",
        "print(\"\\nCluster Characteristics:\")\n",
        "cluster_stats = metrics_df.groupby('Cluster')[clustering_features].agg([\n",
        "    ('mean', 'mean'),\n",
        "    ('min', 'min'),\n",
        "    ('max', 'max')\n",
        "]).round(2)\n",
        "print(cluster_stats)\n",
        "\n",
        "\n",
        "# Create visualizations\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "\n",
        "# Plot 1: CV vs Average Interval\n",
        "plt.subplot(2, 2, 1)\n",
        "scatter = plt.scatter(metrics_df['CV'], metrics_df['Average Interval'],\n",
        "                     c=metrics_df['Cluster'], cmap='viridis', alpha=0.6)\n",
        "plt.xlabel('Coefficient of Variation (CV)')\n",
        "plt.ylabel('Average Interval (days)')\n",
        "plt.title('CV vs Average Interval by Cluster')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "\n",
        "# Plot 2: Average Demand vs Average Order Size\n",
        "plt.subplot(2, 2, 2)\n",
        "scatter = plt.scatter(metrics_df['Average Demand'], metrics_df['Average Order Size'],\n",
        "                     c=metrics_df['Cluster'], cmap='viridis', alpha=0.6)\n",
        "plt.xlabel('Average Demand')\n",
        "plt.ylabel('Average Order Size')\n",
        "plt.title('Average Demand vs Order Size by Cluster')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "\n",
        "# Plot 3: CV vs Average Demand\n",
        "plt.subplot(2, 2, 3)\n",
        "scatter = plt.scatter(metrics_df['CV'], metrics_df['Average Demand'],\n",
        "                     c=metrics_df['Cluster'], cmap='viridis', alpha=0.6)\n",
        "plt.xlabel('Coefficient of Variation (CV)')\n",
        "plt.ylabel('Average Demand')\n",
        "plt.title('CV vs Average Demand')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "\n",
        "# Plot 4: Cluster Characteristics Heatmap\n",
        "plt.subplot(2, 2, 4)\n",
        "cluster_means = metrics_df.groupby('Cluster')[clustering_features].mean()\n",
        "sns.heatmap(cluster_means, annot=True, cmap='YlOrRd', fmt='.2f')\n",
        "plt.title('Cluster Characteristics')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'clustering_analysis.png'))\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Print detailed cluster descriptions\n",
        "print(\"\\nDetailed Cluster Descriptions:\")\n",
        "for cluster in sorted(metrics_df['Cluster'].unique()):\n",
        "    cluster_data = metrics_df[metrics_df['Cluster'] == cluster]\n",
        "    print(f\"\\nCluster {cluster} ({len(cluster_data)} products):\")\n",
        "    print(f\"CV: {cluster_data['CV'].mean():.2f} (±{cluster_data['CV'].std():.2f})\")\n",
        "    print(f\"Average Interval: {cluster_data['Average Interval'].mean():.2f} days (±{cluster_data['Average Interval'].std():.2f})\")\n",
        "    print(f\"Average Demand: {cluster_data['Average Demand'].mean():.2f} (±{cluster_data['Average Demand'].std():.2f})\")\n",
        "    print(f\"Average Order Size: {cluster_data['Average Order Size'].mean():.2f} (±{cluster_data['Average Order Size'].std():.2f})\")\n",
        "    print(\"Example products:\", \", \".join(cluster_data['Product Name'].head(3).tolist()))\n",
        "\n",
        "\n",
        "def analyze_best_clustering(metrics_df, best_labels, clustering_features):\n",
        "    \"\"\"Analyze and visualize the best clustering results in detail\"\"\"\n",
        "    metrics_df['Cluster'] = best_labels\n",
        "\n",
        "    # Create a directory for the analysis outputs\n",
        "    analysis_dir = os.path.join(output_dir, 'cluster_analysis')\n",
        "    if not os.path.exists(analysis_dir):\n",
        "        os.makedirs(analysis_dir)\n",
        "\n",
        "    # 1. Time Series Plot by Cluster\n",
        "    plt.figure(figsize=(20, 12))\n",
        "    for cluster in sorted(metrics_df['Cluster'].unique()):\n",
        "        cluster_products = metrics_df[metrics_df['Cluster'] == cluster]['Product ID'].values\n",
        "        cluster_data = complete_df[complete_df['Product ID'].isin(cluster_products)]\n",
        "\n",
        "        # Calculate daily average demand for the cluster\n",
        "        daily_demand = cluster_data.groupby('Date')['Customer Order Quantity'].mean()\n",
        "\n",
        "        plt.plot(daily_demand.index, daily_demand.values,\n",
        "                label=f'Cluster {cluster} (n={len(cluster_products)})')\n",
        "\n",
        "    plt.title('Average Daily Demand by Cluster')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Average Demand')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(analysis_dir, 'demand_by_cluster.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Cluster Characteristics Radar Chart\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    # Normalize features for radar chart\n",
        "    scaler = StandardScaler()\n",
        "    normalized_features = scaler.fit_transform(metrics_df[clustering_features])\n",
        "    normalized_df = pd.DataFrame(normalized_features, columns=clustering_features)\n",
        "    normalized_df['Cluster'] = metrics_df['Cluster']\n",
        "\n",
        "    # Calculate mean values for each cluster\n",
        "    cluster_means = normalized_df.groupby('Cluster').mean()\n",
        "\n",
        "    # Prepare radar chart\n",
        "    angles = np.linspace(0, 2*np.pi, len(clustering_features), endpoint=False)\n",
        "    angles = np.concatenate((angles, [angles[0]]))  # complete the circle\n",
        "\n",
        "    ax = plt.subplot(111, projection='polar')\n",
        "\n",
        "    for cluster in cluster_means.index:\n",
        "        values = cluster_means.loc[cluster].values\n",
        "        values = np.concatenate((values, [values[0]]))  # complete the circle\n",
        "        ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {cluster}')\n",
        "        ax.fill(angles, values, alpha=0.25)\n",
        "\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(clustering_features)\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
        "    plt.title('Cluster Characteristics Radar Chart')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(analysis_dir, 'cluster_radar.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Detailed Statistics Table\n",
        "    detailed_stats = metrics_df.groupby('Cluster').agg({\n",
        "        'Product ID': 'count',\n",
        "        'Average Demand': ['mean', 'std', 'min', 'max'],\n",
        "        'CV': ['mean', 'std'],\n",
        "        'Zero Percentage': ['mean', 'std'],\n",
        "        'Demand Frequency': ['mean', 'std'],\n",
        "        'Average Order Size': ['mean', 'std'],\n",
        "        'Peak to Average': ['mean', 'std']\n",
        "    }).round(2)\n",
        "\n",
        "    detailed_stats.to_csv(os.path.join(analysis_dir, 'detailed_cluster_stats.csv'))\n",
        "\n",
        "    # 4. Box Plots for Key Metrics\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    key_metrics = ['Average Demand', 'CV', 'Zero Percentage', 'Average Order Size']\n",
        "\n",
        "    for i, metric in enumerate(key_metrics, 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        sns.boxplot(x='Cluster', y=metric, data=metrics_df)\n",
        "        plt.title(f'{metric} Distribution by Cluster')\n",
        "        plt.xticks(rotation=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(analysis_dir, 'cluster_boxplots.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Scatter Matrix of Key Features\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    scatter_features = ['Average Demand', 'CV', 'Zero Percentage', 'Average Order Size']\n",
        "    sns.pairplot(metrics_df, vars=scatter_features, hue='Cluster', diag_kind='kde')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(analysis_dir, 'feature_scatter_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 6. Summary Report with Complete Product Lists\n",
        "    with open(os.path.join(analysis_dir, 'cluster_summary.txt'), 'w') as f:\n",
        "        f.write(\"Cluster Analysis Summary\\n\")\n",
        "        f.write(\"=======================\\n\\n\")\n",
        "\n",
        "        for cluster in sorted(metrics_df['Cluster'].unique()):\n",
        "            cluster_data = metrics_df[metrics_df['Cluster'] == cluster]\n",
        "            f.write(f\"\\nCluster {cluster}:\\n\")\n",
        "            f.write(f\"Number of products: {len(cluster_data)}\\n\")\n",
        "            f.write(f\"Average demand: {cluster_data['Average Demand'].mean():.2f}\\n\")\n",
        "            f.write(f\"Average CV: {cluster_data['CV'].mean():.2f}\\n\")\n",
        "            f.write(f\"Average order frequency: {cluster_data['Demand Frequency'].mean():.2%}\\n\")\n",
        "\n",
        "            f.write(\"\\nKey Characteristics:\\n\")\n",
        "            f.write(f\"- Order size range: {cluster_data['Average Order Size'].min():.1f} - {cluster_data['Average Order Size'].max():.1f}\\n\")\n",
        "            f.write(f\"- Zero demand days: {cluster_data['Zero Percentage'].mean():.1f}%\\n\")\n",
        "            f.write(f\"- Peak to average ratio: {cluster_data['Peak to Average'].mean():.1f}\\n\")\n",
        "\n",
        "            f.write(\"\\nComplete Product List:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            # Sort products by average demand within cluster\n",
        "            sorted_products = cluster_data.sort_values('Average Demand', ascending=False)\n",
        "            for idx, row in sorted_products.iterrows():\n",
        "                f.write(f\"Product: {row['Product Name']}\\n\")\n",
        "                f.write(f\"  - Average Demand: {row['Average Demand']:.2f}\\n\")\n",
        "                f.write(f\"  - CV: {row['CV']:.2f}\\n\")\n",
        "                f.write(f\"  - Order Frequency: {row['Demand Frequency']:.2%}\\n\")\n",
        "                f.write(f\"  - Average Order Size: {row['Average Order Size']:.2f}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "    # Add the new radar chart with the analysis_dir parameter\n",
        "    create_detailed_radar_chart(metrics_df, clustering_features, analysis_dir)\n",
        "\n",
        "\n",
        "def create_detailed_radar_chart(metrics_df, clustering_features, output_directory):\n",
        "    \"\"\"Create a radar chart showing mean, max, and min values for each cluster\"\"\"\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    # Create two subplots side by side\n",
        "    ax1 = plt.subplot(121, projection='polar')\n",
        "    ax2 = plt.subplot(122, projection='polar')\n",
        "\n",
        "    # 1. Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    normalized_features = scaler.fit_transform(metrics_df[clustering_features])\n",
        "    normalized_df = pd.DataFrame(normalized_features, columns=clustering_features)\n",
        "    normalized_df['Cluster'] = metrics_df['Cluster']\n",
        "\n",
        "    # 2. Calculate statistics for each cluster\n",
        "    cluster_means = normalized_df.groupby('Cluster').mean()\n",
        "    cluster_max = normalized_df.groupby('Cluster').max()\n",
        "    cluster_min = normalized_df.groupby('Cluster').min()\n",
        "\n",
        "    # 3. Prepare angles\n",
        "    angles = np.linspace(0, 2*np.pi, len(clustering_features), endpoint=False)\n",
        "    angles = np.concatenate((angles, [angles[0]]))  # complete the circle\n",
        "\n",
        "    # Colors for each cluster\n",
        "    colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
        "\n",
        "    # 4. Plot mean values (left subplot)\n",
        "    for idx, cluster in enumerate(cluster_means.index):\n",
        "        values = cluster_means.loc[cluster].values\n",
        "        values = np.concatenate((values, [values[0]]))\n",
        "        ax1.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {cluster}', color=colors[idx])\n",
        "        ax1.fill(angles, values, alpha=0.25, color=colors[idx])\n",
        "\n",
        "    # Set chart attributes for mean values\n",
        "    ax1.set_xticks(angles[:-1])\n",
        "    ax1.set_xticklabels(clustering_features, size=8)\n",
        "    ax1.set_title('Cluster Means')\n",
        "    ax1.legend(loc='upper right', bbox_to_anchor=(0.3, 0.3))\n",
        "\n",
        "    # 5. Plot mean, max, and min values (right subplot)\n",
        "    line_styles = ['-', '--', ':']  # for mean, max, min\n",
        "\n",
        "    for idx, cluster in enumerate(cluster_means.index):\n",
        "        # Plot mean\n",
        "        mean_values = cluster_means.loc[cluster].values\n",
        "        mean_values = np.concatenate((mean_values, [mean_values[0]]))\n",
        "        ax2.plot(angles, mean_values, 'o-', linewidth=2,\n",
        "                label=f'Cluster {cluster} (Mean)', color=colors[idx])\n",
        "\n",
        "        # Plot max\n",
        "        max_values = cluster_max.loc[cluster].values\n",
        "        max_values = np.concatenate((max_values, [max_values[0]]))\n",
        "        ax2.plot(angles, max_values, 's--', linewidth=1,\n",
        "                label=f'Cluster {cluster} (Max)', color=colors[idx])\n",
        "\n",
        "        # Plot min\n",
        "        min_values = cluster_min.loc[cluster].values\n",
        "        min_values = np.concatenate((min_values, [min_values[0]]))\n",
        "        ax2.plot(angles, min_values, '^:', linewidth=1,\n",
        "                label=f'Cluster {cluster} (Min)', color=colors[idx])\n",
        "\n",
        "        # Fill between min and max\n",
        "        ax2.fill_between(angles, min_values, max_values, alpha=0.1, color=colors[idx])\n",
        "\n",
        "    # Set chart attributes for detailed values\n",
        "    ax2.set_xticks(angles[:-1])\n",
        "    ax2.set_xticklabels(clustering_features, size=8)\n",
        "    ax2.set_title('Cluster Means with Min-Max Ranges')\n",
        "    ax2.legend(loc='upper right', bbox_to_anchor=(0.3, 0.3))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_directory, 'detailed_cluster_radar.png'),\n",
        "                bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# After getting best_labels, call the analysis function\n",
        "analyze_best_clustering(metrics_df, best_labels, clustering_features)\n",
        "\n",
        "\n",
        "# Create demand pattern classification plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "\n",
        "# Standard thresholds\n",
        "cv2_threshold = 0.49  # Square of 0.7\n",
        "adi_threshold = 1.32\n",
        "\n",
        "\n",
        "plt.axhline(y=adi_threshold, color='gray', linestyle='--', alpha=0.3)\n",
        "plt.axvline(x=cv2_threshold, color='gray', linestyle='--', alpha=0.3)\n",
        "\n",
        "\n",
        "# Add region labels\n",
        "plt.text(0.25, 0.5, 'Smooth', ha='center', va='center', alpha=0.3)\n",
        "plt.text(1.0, 0.5, 'Erratic', ha='center', va='center', alpha=0.3)\n",
        "plt.text(0.25, 2, 'Intermittent', ha='center', va='center', alpha=0.3)\n",
        "plt.text(1.0, 2, 'Lumpy', ha='center', va='center', alpha=0.3)\n",
        "\n",
        "\n",
        "# Calculate CV squared for plotting\n",
        "cv_squared = metrics_df['CV'] ** 2\n",
        "\n",
        "\n",
        "# Create scatter plot\n",
        "scatter = plt.scatter(cv_squared,\n",
        "                     metrics_df['Average Interval'],\n",
        "                     c=metrics_df['Cluster'],\n",
        "                     cmap='viridis',\n",
        "                     alpha=0.6)\n",
        "\n",
        "\n",
        "plt.xlabel('Coefficient of Variation Squared (CV²)')\n",
        "plt.ylabel('Average Interval (days)')\n",
        "plt.title('Demand Pattern Classification')\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "\n",
        "# Set axis limits based on data with padding\n",
        "x_max = max(2, cv_squared.max() * 1.1)\n",
        "y_max = max(3, metrics_df['Average Interval'].max() * 1.1)\n",
        "plt.xlim(-0.1, x_max)\n",
        "plt.ylim(-0.1, y_max)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'demand_pattern_classification.png'))\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Create final summary with pattern classifications\n",
        "def classify_pattern(cv, interval):\n",
        "    \"\"\"\n",
        "    Classify demand pattern based on CV² and ADI thresholds\n",
        "    CV² threshold = 0.49\n",
        "    ADI threshold = 1.32\n",
        "    \"\"\"\n",
        "    cv_squared = cv ** 2\n",
        "    if cv_squared <= 0.49:\n",
        "        if interval <= 1.32:\n",
        "            return 'Smooth'\n",
        "        else:\n",
        "            return 'Intermittent'\n",
        "    else:\n",
        "        if interval <= 1.32:\n",
        "            return 'Erratic'\n",
        "        else:\n",
        "            return 'Lumpy'\n",
        "\n",
        "\n",
        "# Create final summary DataFrame\n",
        "final_summary = pd.DataFrame({\n",
        "    'Product ID': metrics_df['Product ID'],\n",
        "    'Product Name': metrics_df['Product Name'],\n",
        "    'Cluster': metrics_df['Cluster'],\n",
        "    'CV': metrics_df['CV'],\n",
        "    'CV_Squared': metrics_df['CV'] ** 2,\n",
        "    'Average Interval': metrics_df['Average Interval'],\n",
        "    'Average Demand': metrics_df['Average Demand'],\n",
        "    'Average Order Size': metrics_df['Average Order Size']\n",
        "})\n",
        "\n",
        "\n",
        "# Add pattern classification\n",
        "final_summary['Pattern'] = final_summary.apply(\n",
        "    lambda row: classify_pattern(row['CV'], row['Average Interval']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "\n",
        "# Save final summary\n",
        "final_summary.to_csv(os.path.join(output_dir, 'final_pattern_summary.csv'), index=False)\n",
        "\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nDemand Pattern Distribution:\")\n",
        "pattern_counts = final_summary['Pattern'].value_counts()\n",
        "for pattern, count in pattern_counts.items():\n",
        "    print(f\"{pattern}: {count} products ({count/len(final_summary)*100:.1f}%)\")\n",
        "\n",
        "\n",
        "print(\"\\nCluster and Pattern Cross-tabulation:\")\n",
        "cross_tab = pd.crosstab(final_summary['Cluster'], final_summary['Pattern'])\n",
        "print(cross_tab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Un7HEdQsQd-f",
        "outputId": "aaa0fb14-2329-4f89-d983-69b4609cbe7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using clustering features: ['CV', 'Average Interval', 'Average Demand', 'Average Order Size']\n",
            "\n",
            "Feature ranges after scaling:\n",
            "CV: -1.17 to 27.31\n",
            "Average Interval: -0.71 to 52.04\n",
            "Average Demand: -0.44 to 32.75\n",
            "Average Order Size: -0.49 to 81.34\n",
            "\n",
            "Testing K-means clustering:\n",
            "K=3: Silhouette Score = 0.848, Cluster sizes = [ 5  1 78]\n",
            "K=4: Silhouette Score = 0.828, Cluster sizes = [78  1  4  1]\n",
            "K=5: Silhouette Score = 0.756, Cluster sizes = [74  1  2  1  6]\n",
            "K=6: Silhouette Score = 0.737, Cluster sizes = [74  1  1  1  5  2]\n",
            "\n",
            "Testing DBSCAN clustering:\n",
            "eps=0.5: Silhouette Score = 0.270\n",
            "Cluster sizes (including noise points): {-1: 23, 0: 48, 1: 5, 2: 3, 3: 5}\n",
            "\n",
            "Testing Hierarchical clustering:\n",
            "K=3: Silhouette Score = 0.848, Cluster sizes = [ 5  1 78]\n",
            "K=4: Silhouette Score = 0.828, Cluster sizes = [78  1  4  1]\n",
            "K=5: Silhouette Score = 0.661, Cluster sizes = [ 4  7 71  1  1]\n",
            "K=6: Silhouette Score = 0.663, Cluster sizes = [ 3  7 71  1  1  1]\n",
            "\n",
            "Best clustering method: KMeans-3\n",
            "Best silhouette score: 0.848\n",
            "\n",
            "Cluster Characteristics:\n",
            "            CV              Average Interval               Average Demand  \\\n",
            "          mean   min    max             mean    min    max           mean   \n",
            "Cluster                                                                     \n",
            "0        19.27  8.72  29.54            40.20  21.37  86.29           0.41   \n",
            "1         4.43  4.43   4.43             2.86   2.86   2.86         586.45   \n",
            "2         2.65  1.23  10.09             3.16   1.24  16.10          13.54   \n",
            "\n",
            "                        Average Order Size                    \n",
            "            min     max               mean      min      max  \n",
            "Cluster                                                       \n",
            "0          0.23    0.79              44.21     7.53   105.88  \n",
            "1        586.45  586.45            2525.51  2525.51  2525.51  \n",
            "2          0.09   64.66              30.03     3.08   163.64  \n",
            "\n",
            "Detailed Cluster Descriptions:\n",
            "\n",
            "Cluster 0 (5 products):\n",
            "CV: 19.27 (±7.92)\n",
            "Average Interval: 40.20 days (±26.71)\n",
            "Average Demand: 0.41 (±0.23)\n",
            "Average Order Size: 44.21 (±46.04)\n",
            "Example products: Material_29, Material_37, Material_26\n",
            "\n",
            "Cluster 1 (1 products):\n",
            "CV: 4.43 (±nan)\n",
            "Average Interval: 2.86 days (±nan)\n",
            "Average Demand: 586.45 (±nan)\n",
            "Average Order Size: 2525.51 (±nan)\n",
            "Example products: Material_81\n",
            "\n",
            "Cluster 2 (78 products):\n",
            "CV: 2.65 (±1.44)\n",
            "Average Interval: 3.16 days (±2.95)\n",
            "Average Demand: 13.54 (±13.71)\n",
            "Average Order Size: 30.03 (±28.43)\n",
            "Example products: Material_4, Material_12, Material_14\n",
            "\n",
            "Demand Pattern Distribution:\n",
            "Lumpy: 78 products (92.9%)\n",
            "Erratic: 6 products (7.1%)\n",
            "\n",
            "Cluster and Pattern Cross-tabulation:\n",
            "Pattern  Erratic  Lumpy\n",
            "Cluster                \n",
            "0              0      5\n",
            "1              0      1\n",
            "2              6     72\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x2000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for forecasting\n",
        "weekly_df = df.groupby(['Date', 'Product ID'])['Customer Order Quantity'].sum().reset_index()\n",
        "weekly_df.columns = [\"ds\", \"unique_id\", \"y\"]\n",
        "weekly_df[\"ds\"] = pd.to_datetime(weekly_df[\"ds\"])\n",
        "\n",
        "# Split into train and test\n",
        "train = weekly_df[weekly_df.ds <= '2024-06-30']\n",
        "test = weekly_df[weekly_df.ds > '2024-06-30']\n",
        "\n",
        "# Save test data\n",
        "test.to_csv(os.path.join(output_dir, 'test_data.csv'), index=False)\n",
        "\n",
        "# Define forecast settings\n",
        "season_length = 24\n",
        "horizon = len(test['ds'].unique())\n",
        "freq = \"W-MON\"\n",
        "\n",
        "# Define forecasting models\n",
        "models = [\n",
        "    SeasonalNaive(season_length=season_length),\n",
        "    AutoTheta(season_length=season_length, decomposition_type=\"additive\", model=\"STM\"),\n",
        "    CrostonClassic(),\n",
        "    CrostonOptimized(),\n",
        "    ADIDA(),\n",
        "    CrostonSBA(),\n",
        "    IMAPA(),\n",
        "    TSB(alpha_d=0.5, alpha_p=0.5)\n",
        "]\n",
        "\n",
        "sf = StatsForecast(models=models, freq=freq, n_jobs=-1)\n",
        "\n",
        "# Generate forecasts\n",
        "forecasts_df = sf.forecast(df=train, h=horizon)\n",
        "\n",
        "# Additional forecasting methods\n",
        "def moving_average_forecast(df, window=4, horizon=4):\n",
        "    forecasts = []\n",
        "    for uid in df[\"unique_id\"].unique():\n",
        "        series = df[df[\"unique_id\"] == uid].sort_values(\"ds\")\n",
        "        past_values = series[\"y\"].rolling(window=window).mean().iloc[-1] if len(series) >= window else series[\"y\"].mean()\n",
        "        future_dates = pd.date_range(start=series[\"ds\"].max(), periods=horizon + 1, freq=\"W-MON\")[1:]\n",
        "        forecast_df = pd.DataFrame({\"unique_id\": uid, \"ds\": future_dates, \"moving_avg_forecast\": [past_values] * horizon})\n",
        "        forecasts.append(forecast_df)\n",
        "    return pd.concat(forecasts, ignore_index=True)\n",
        "\n",
        "def weighted_moving_average_forecast(df, window=4, horizon=4):\n",
        "    forecasts = []\n",
        "    weights = np.arange(1, window + 1) / sum(np.arange(1, window + 1))\n",
        "    for uid in df[\"unique_id\"].unique():\n",
        "        series = df[df[\"unique_id\"] == uid].sort_values(\"ds\")\n",
        "        if len(series) >= window:\n",
        "            past_values = np.dot(series[\"y\"].iloc[-window:], weights)\n",
        "        else:\n",
        "            past_values = series[\"y\"].mean()\n",
        "        future_dates = pd.date_range(start=series[\"ds\"].max(), periods=horizon + 1, freq=\"W-MON\")[1:]\n",
        "        forecast_df = pd.DataFrame({\"unique_id\": uid, \"ds\": future_dates, \"wma_forecast\": [past_values] * horizon})\n",
        "        forecasts.append(forecast_df)\n",
        "    return pd.concat(forecasts, ignore_index=True)\n",
        "\n",
        "def exponential_smoothing_forecast(df, alpha=0.3, horizon=4):\n",
        "    forecasts = []\n",
        "    for uid in df[\"unique_id\"].unique():\n",
        "        series = df[df[\"unique_id\"] == uid].sort_values(\"ds\")[\"y\"]\n",
        "        if len(series) > 1:\n",
        "            ses_value = series.iloc[0]\n",
        "            for val in series:\n",
        "                ses_value = alpha * val + (1 - alpha) * ses_value\n",
        "        else:\n",
        "            ses_value = series.mean()\n",
        "        future_dates = pd.date_range(start=df[df[\"unique_id\"] == uid][\"ds\"].max(), periods=horizon + 1, freq=\"W-MON\")[1:]\n",
        "        forecast_df = pd.DataFrame({\"unique_id\": uid, \"ds\": future_dates, \"ses_forecast\": [ses_value] * horizon})\n",
        "        forecasts.append(forecast_df)\n",
        "    return pd.concat(forecasts, ignore_index=True)\n",
        "\n",
        "# Generate additional forecasts\n",
        "moving_avg_df = moving_average_forecast(train, window=4, horizon=horizon)\n",
        "wma_df = weighted_moving_average_forecast(train, window=4, horizon=horizon)\n",
        "ses_df = exponential_smoothing_forecast(train, alpha=0.3, horizon=horizon)\n",
        "\n",
        "# Merge all forecasts\n",
        "forecasts_df = forecasts_df.merge(moving_avg_df, on=['unique_id', 'ds'], how='left')\n",
        "forecasts_df = forecasts_df.merge(wma_df, on=['unique_id', 'ds'], how='left')\n",
        "forecasts_df = forecasts_df.merge(ses_df, on=['unique_id', 'ds'], how='left')\n",
        "\n",
        "# Save forecasts\n",
        "forecasts_df.to_csv(os.path.join(output_dir, \"statistical_full_data_forecast_with_MA_WMA_SES.csv\"), index=False)\n",
        "\n",
        "print(\"All necessary files have been created in the 'Corona' directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq9KnP9pWUQF",
        "outputId": "cd1eba66-ac23-4582-bdcd-680b23ce5a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All necessary files have been created in the 'Corona' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the necessary data\n",
        "metrics = pd.read_csv(os.path.join(output_dir, 'final_pattern_summary.csv'))\n",
        "forecasts_df = pd.read_csv(os.path.join(output_dir, 'statistical_full_data_forecast_with_MA_WMA_SES.csv'))\n",
        "test = pd.read_csv(os.path.join(output_dir, 'test_data.csv'))\n",
        "\n",
        "# Merge cluster assignments with forecasts and test data\n",
        "forecasts_df = forecasts_df.merge(metrics[['Product ID', 'Cluster']], left_on='unique_id', right_on='Product ID', how='left')\n",
        "test = test.merge(metrics[['Product ID', 'Cluster']], left_on='unique_id', right_on='Product ID', how='left')\n",
        "\n",
        "def calculate_cluster_rmse(test, forecasts_df):\n",
        "    merged_df = pd.merge(test, forecasts_df, on=['unique_id', 'ds', 'Cluster'])\n",
        "    rmse_scores = []\n",
        "\n",
        "    model_columns = [col for col in forecasts_df.columns if col not in ['unique_id', 'ds', 'Cluster', 'Product ID']]\n",
        "\n",
        "    for model in model_columns:\n",
        "        cluster_rmse = merged_df.groupby('Cluster').apply(lambda x: np.sqrt(np.mean((x['y'] - x[model])**2)), include_groups=False)\n",
        "        rmse_scores.append(pd.DataFrame({'Cluster': cluster_rmse.index, 'model': model, 'rmse': cluster_rmse.values}))\n",
        "\n",
        "    return pd.concat(rmse_scores, ignore_index=True)\n",
        "\n",
        "cluster_rmse_df = calculate_cluster_rmse(test, forecasts_df)\n",
        "\n",
        "\n",
        "# Select the best model per cluster\n",
        "def select_best_model_per_cluster(metrics_df):\n",
        "    return metrics_df.loc[metrics_df.groupby('Cluster')['rmse'].idxmin()][['Cluster', 'model', 'rmse']]\n",
        "\n",
        "best_models_per_cluster = select_best_model_per_cluster(cluster_rmse_df)\n",
        "\n",
        "# Calculate average RMSE for each cluster using all products and their RMSEs\n",
        "average_rmse_per_cluster = cluster_rmse_df.groupby('Cluster')['rmse'].mean().reset_index()\n",
        "average_rmse_per_cluster.columns = ['Cluster', 'Average RMSE']\n",
        "\n",
        "# Combine best models and average RMSE\n",
        "final_results = best_models_per_cluster.merge(average_rmse_per_cluster, on='Cluster')\n",
        "\n",
        "\n",
        "# Save the results\n",
        "final_results.to_csv(os.path.join(output_dir, \"best_forecasting_models_per_cluster.csv\"), index=False)\n",
        "\n",
        "print(final_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNrs0Md7VfU2",
        "outputId": "bb201df7-f4eb-4bc2-ce21-d87b70cd03e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Cluster           model        rmse  Average RMSE\n",
            "0        0   SeasonalNaive    1.732051     81.753186\n",
            "1        1             TSB  925.895152   1082.543126\n",
            "2        2  CrostonClassic   41.534753     47.028832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "1hNf2EFan-oo",
        "outputId": "3b118262-c3be-4bef-b93f-7249536e0104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Product ID Product Name  Average Demand         CV  Average Interval  \\\n",
              "0   000161032   Material_4       47.747191   1.356218          1.345598   \n",
              "1   000179754  Material_12        4.817416   4.271823          3.720000   \n",
              "2   000179758  Material_14        0.547753   5.862913         16.098361   \n",
              "3   000180631  Material_17        4.548689   2.367486          2.672727   \n",
              "4   000180632  Material_19       20.960674   1.466526          1.476395   \n",
              "..        ...          ...             ...        ...               ...   \n",
              "79  5551O1476  Material_62        5.140449   2.444614          2.915254   \n",
              "80  5551O2494  Material_73        5.742509   3.054945          2.826087   \n",
              "81  5551O2515  Material_75        0.253745  21.855419         21.368421   \n",
              "82  5551O2518  Material_76       23.842697   2.732534          2.400000   \n",
              "83  5551O3273  Material_79       30.179775   2.514716          2.417249   \n",
              "\n",
              "    Zero Percentage  Demand Frequency  Average Order Size  Peak to Average  \\\n",
              "0         28.651685          0.713483           66.921260         9.424638   \n",
              "1         74.157303          0.258427           18.641304       112.508455   \n",
              "2         94.194757          0.058052            9.435484       109.538462   \n",
              "3         63.857678          0.361423           12.585492        27.480445   \n",
              "4         34.456929          0.655431           31.980000        12.404181   \n",
              "..              ...               ...                 ...              ...   \n",
              "79        66.760300          0.332397           15.464789        54.859016   \n",
              "80        65.449438          0.345506           16.620596        64.780042   \n",
              "81        98.127341          0.018727           13.550000       693.608856   \n",
              "82        59.644195          0.403558           59.081206        70.461828   \n",
              "83        59.737828          0.402622           74.958140        63.221147   \n",
              "\n",
              "    Demand Variability  Interval CV  Cluster  \n",
              "0             0.482497     0.591513        2  \n",
              "1             1.567808     0.847821        2  \n",
              "2             2.178872     0.944663        2  \n",
              "3             0.863758     0.913448        2  \n",
              "4             0.573975     0.616110        2  \n",
              "..                 ...          ...      ...  \n",
              "79            0.877297     0.575219        2  \n",
              "80            1.201124     0.807161        2  \n",
              "81            8.270066     1.354227        0  \n",
              "82            1.038591     0.928483        2  \n",
              "83            0.844778     0.535311        2  \n",
              "\n",
              "[84 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-537c45ab-3fc8-4620-b341-ee4ed4ff5b8d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product ID</th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Average Demand</th>\n",
              "      <th>CV</th>\n",
              "      <th>Average Interval</th>\n",
              "      <th>Zero Percentage</th>\n",
              "      <th>Demand Frequency</th>\n",
              "      <th>Average Order Size</th>\n",
              "      <th>Peak to Average</th>\n",
              "      <th>Demand Variability</th>\n",
              "      <th>Interval CV</th>\n",
              "      <th>Cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000161032</td>\n",
              "      <td>Material_4</td>\n",
              "      <td>47.747191</td>\n",
              "      <td>1.356218</td>\n",
              "      <td>1.345598</td>\n",
              "      <td>28.651685</td>\n",
              "      <td>0.713483</td>\n",
              "      <td>66.921260</td>\n",
              "      <td>9.424638</td>\n",
              "      <td>0.482497</td>\n",
              "      <td>0.591513</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000179754</td>\n",
              "      <td>Material_12</td>\n",
              "      <td>4.817416</td>\n",
              "      <td>4.271823</td>\n",
              "      <td>3.720000</td>\n",
              "      <td>74.157303</td>\n",
              "      <td>0.258427</td>\n",
              "      <td>18.641304</td>\n",
              "      <td>112.508455</td>\n",
              "      <td>1.567808</td>\n",
              "      <td>0.847821</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000179758</td>\n",
              "      <td>Material_14</td>\n",
              "      <td>0.547753</td>\n",
              "      <td>5.862913</td>\n",
              "      <td>16.098361</td>\n",
              "      <td>94.194757</td>\n",
              "      <td>0.058052</td>\n",
              "      <td>9.435484</td>\n",
              "      <td>109.538462</td>\n",
              "      <td>2.178872</td>\n",
              "      <td>0.944663</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000180631</td>\n",
              "      <td>Material_17</td>\n",
              "      <td>4.548689</td>\n",
              "      <td>2.367486</td>\n",
              "      <td>2.672727</td>\n",
              "      <td>63.857678</td>\n",
              "      <td>0.361423</td>\n",
              "      <td>12.585492</td>\n",
              "      <td>27.480445</td>\n",
              "      <td>0.863758</td>\n",
              "      <td>0.913448</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000180632</td>\n",
              "      <td>Material_19</td>\n",
              "      <td>20.960674</td>\n",
              "      <td>1.466526</td>\n",
              "      <td>1.476395</td>\n",
              "      <td>34.456929</td>\n",
              "      <td>0.655431</td>\n",
              "      <td>31.980000</td>\n",
              "      <td>12.404181</td>\n",
              "      <td>0.573975</td>\n",
              "      <td>0.616110</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>5551O1476</td>\n",
              "      <td>Material_62</td>\n",
              "      <td>5.140449</td>\n",
              "      <td>2.444614</td>\n",
              "      <td>2.915254</td>\n",
              "      <td>66.760300</td>\n",
              "      <td>0.332397</td>\n",
              "      <td>15.464789</td>\n",
              "      <td>54.859016</td>\n",
              "      <td>0.877297</td>\n",
              "      <td>0.575219</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>5551O2494</td>\n",
              "      <td>Material_73</td>\n",
              "      <td>5.742509</td>\n",
              "      <td>3.054945</td>\n",
              "      <td>2.826087</td>\n",
              "      <td>65.449438</td>\n",
              "      <td>0.345506</td>\n",
              "      <td>16.620596</td>\n",
              "      <td>64.780042</td>\n",
              "      <td>1.201124</td>\n",
              "      <td>0.807161</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>5551O2515</td>\n",
              "      <td>Material_75</td>\n",
              "      <td>0.253745</td>\n",
              "      <td>21.855419</td>\n",
              "      <td>21.368421</td>\n",
              "      <td>98.127341</td>\n",
              "      <td>0.018727</td>\n",
              "      <td>13.550000</td>\n",
              "      <td>693.608856</td>\n",
              "      <td>8.270066</td>\n",
              "      <td>1.354227</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>5551O2518</td>\n",
              "      <td>Material_76</td>\n",
              "      <td>23.842697</td>\n",
              "      <td>2.732534</td>\n",
              "      <td>2.400000</td>\n",
              "      <td>59.644195</td>\n",
              "      <td>0.403558</td>\n",
              "      <td>59.081206</td>\n",
              "      <td>70.461828</td>\n",
              "      <td>1.038591</td>\n",
              "      <td>0.928483</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>5551O3273</td>\n",
              "      <td>Material_79</td>\n",
              "      <td>30.179775</td>\n",
              "      <td>2.514716</td>\n",
              "      <td>2.417249</td>\n",
              "      <td>59.737828</td>\n",
              "      <td>0.402622</td>\n",
              "      <td>74.958140</td>\n",
              "      <td>63.221147</td>\n",
              "      <td>0.844778</td>\n",
              "      <td>0.535311</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84 rows × 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-537c45ab-3fc8-4620-b341-ee4ed4ff5b8d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-537c45ab-3fc8-4620-b341-ee4ed4ff5b8d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-537c45ab-3fc8-4620-b341-ee4ed4ff5b8d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ad2312c3-69d2-4c2a-8b6d-c8af510c2960\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ad2312c3-69d2-4c2a-8b6d-c8af510c2960')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ad2312c3-69d2-4c2a-8b6d-c8af510c2960 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f8e0c9b4-4ddc-4dbb-8abc-eed3ffa3bfe6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f8e0c9b4-4ddc-4dbb-8abc-eed3ffa3bfe6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_df",
              "summary": "{\n  \"name\": \"metrics_df\",\n  \"rows\": 84,\n  \"fields\": [\n    {\n      \"column\": \"Product ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 84,\n        \"samples\": [\n          \"5551O1138\",\n          \"000161032\",\n          \"555179781\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 84,\n        \"samples\": [\n          \"Material_54\",\n          \"Material_4\",\n          \"Material_16\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Average Demand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 64.04976721663677,\n        \"min\": 0.08801498127340847,\n        \"max\": 586.4475655430712,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          20.741573033707866,\n          47.747191011235955,\n          3.4691011235955056\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.5393981473094405,\n        \"min\": 1.2261775599739289,\n        \"max\": 29.53582654157099,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          2.3267632304021886,\n          1.3562183722528676,\n          2.416421579223522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Average Interval\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.964117279349068,\n        \"min\": 1.2359281437125749,\n        \"max\": 86.28571428571429,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          3.5119453924914676,\n          1.3455978975032852,\n          2.3008849557522124\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Zero Percentage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20.752123212173522,\n        \"min\": 21.722846441947567,\n        \"max\": 99.625468164794,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          34.45692883895131,\n          53.08988764044944,\n          57.490636704119844\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Demand Frequency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20752123212173523,\n        \"min\": 0.003745318352059925,\n        \"max\": 0.7827715355805244,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          0.6554307116104869,\n          0.4691011235955056,\n          0.4250936329588015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Average Order Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 273.76743269930944,\n        \"min\": 3.081168831168831,\n        \"max\": 2525.5080645161293,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          75.34693877551021,\n          66.92125984251969,\n          8.178807947019868\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Peak to Average\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 148.15428736726756,\n        \"min\": 8.577125450013847,\n        \"max\": 963.1656441717784,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          39.91982665222102,\n          9.424638192728557,\n          50.15708502024292\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Demand Variability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7113558617729456,\n        \"min\": 0.4524648117786337,\n        \"max\": 11.093489108944546,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          0.8481641816066849,\n          0.4824970422239949,\n          0.9144529455001534\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Interval CV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4119037302789916,\n        \"min\": 0.3890447039980106,\n        \"max\": 3.3475178210254084,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          0.6547467143060322,\n          0.5915131848914195,\n          0.7031417239957167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cluster\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}